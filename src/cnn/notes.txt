HOW TO RUN:

1. Training:
	$CAFFE_ROOT/build/tools/caffe train -solver data/cnn/city/final/city_quick_solver_filters.prototxt  -gpu 1
   For HDF5:
	$CAFFE_ROOT/build/tools/caffe train -solver data/cnn/city/final/hdf5_solver.prototxt  -gpu 1
NOTE: The batch size is kept as 1 in the prototxt file. The number of test examples are mentioned in the solver ( test_iter==number of test examples)
NOTE: The train and test files are names train-rgb-b.txt and test-rgb-b.txt respectively inside the prototxt file.
2. Extracting Features:
	./build/cnn/extract_features_text data/cnn/city/final/city_quick_iter_4000.caffemodel data/cnn/city/final/city_quick_train_test_filters.prototxt conv1 data/cnn/city/features 1 lmdb

3. Visualization
	ipython notebook src/cnn/visualization.ipynb
NOTE: It uses deploy_python.prototxt
4. C++ Deployment
	./build/cnn/predict
NOTE: It uses deploy_cpp.prototxt. The test examples are listed in the file data/cnn/city-data/test-rgb-b.txt. You need to mention this file in the deploy_cpp.prototxt along with number of batches == number of test examples

5. Python Deployment
	python src/cnn/DeploymentPatches.py
NOTE: It uses deploy_python.prototxt. The test example is mentioned in the DeploymentPatches.py file.

* All prototxt files are in data/cnn/city. All test and train text files are in data/cnn/city-data.
** For HDF5, the train and test .h5 files are passed through train and test text files. The training is similar.

HOW TO CREATE TRIN AND TEST FILES FOR TRAINING:
1. Split the data into test and train
ls -d -1 $PWD/*.*>files.txt
awk '{print $0 " 0"}' files.txt > test-rgb-b.txt
